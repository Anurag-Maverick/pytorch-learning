{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch ","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:55:53.264354Z","iopub.execute_input":"2021-06-09T16:55:53.264821Z","iopub.status.idle":"2021-06-09T16:55:54.453308Z","shell.execute_reply.started":"2021-06-09T16:55:53.264751Z","shell.execute_reply":"2021-06-09T16:55:54.452424Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Part 1: Using only Numpy","metadata":{}},{"cell_type":"code","source":"#Dataset\nx = np.array([1,2,3,4], dtype=np.float32)\ny = np.array([2,4,6,8], dtype=np.float32) #y = x^2\n\n#forward pass\ndef forward_pass(x):\n    return x*w\n\n#mse loss\ndef loss_compute(y,yhat):\n    return np.mean((yhat-y)**2)\n\n#gradient computation\n# mse = ((yhat-y)**2)/N\n# dJ/dy = 2(yhat-y)/N\n# dy/dw = x\n# So, dJ/dw = 2x(yhat-y)/N\n#but yhat = x * w\n#So, dJ/dw = 2x(w*x - y)/N\n\ndef gradient(y,yhat,x):\n    return np.mean(np.dot(2*x, (yhat - y)))\n\nlearning_rate = 0.01\nepochs = 100\nw = 0.0\nfor ep in range(epochs):\n    ypred = forward_pass(x)\n    loss = loss_compute(y,ypred)\n    \n    if (ep%10==0):\n        print(\"Loss in epoch {} is {}\".format(ep+1,loss))\n\n    dj_dw = gradient(y,ypred,x)\n    w -= learning_rate*dj_dw","metadata":{"execution":{"iopub.status.busy":"2021-06-09T18:28:23.482986Z","iopub.execute_input":"2021-06-09T18:28:23.483339Z","iopub.status.idle":"2021-06-09T18:28:23.503080Z","shell.execute_reply.started":"2021-06-09T18:28:23.483310Z","shell.execute_reply":"2021-06-09T18:28:23.501809Z"},"trusted":true},"execution_count":95,"outputs":[{"name":"stdout","text":"Loss in epoch 1 is 30.0\nLoss in epoch 11 is 3.297340072094812e-07\nLoss in epoch 21 is 0.0\nLoss in epoch 31 is 0.0\nLoss in epoch 41 is 0.0\nLoss in epoch 51 is 0.0\nLoss in epoch 61 is 0.0\nLoss in epoch 71 is 0.0\nLoss in epoch 81 is 0.0\nLoss in epoch 91 is 0.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Loss is decreasing with more epochs!!","metadata":{}},{"cell_type":"markdown","source":"# Part 2: Using Only Pytorch","metadata":{}},{"cell_type":"code","source":"x = torch.tensor([1,2,3,4], dtype = torch.float32)\ny = torch.tensor([2,4,6,8], dtype = torch.float32)\n\nweights = torch.tensor(0.0, dtype = torch.float32, requires_grad=True)\n\ndef forward_pass(x):\n    return x * weights\n\ndef loss_compute(y,yhat):\n    return ((yhat-y)**2).mean()\n\nlearning_rate = 0.01\nepochs = 100\nfor ep in range(epochs):\n    ypred = forward_pass(x)\n    loss = loss_compute(y,ypred)\n    loss.backward()\n    \n    if (ep%10==0):\n        print(\"Loss in epoch {} is {}\".format(ep+1,loss))\n    \n    with torch.no_grad():\n        weights -= learning_rate * weights.grad\n    weights.grad.zero_()\n    \nprint(\"Predicted value of 7.5 is {}\".format(forward_pass(7.5)))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T18:28:39.938702Z","iopub.execute_input":"2021-06-09T18:28:39.939017Z","iopub.status.idle":"2021-06-09T18:28:39.974724Z","shell.execute_reply.started":"2021-06-09T18:28:39.938993Z","shell.execute_reply":"2021-06-09T18:28:39.973612Z"},"trusted":true},"execution_count":96,"outputs":[{"name":"stdout","text":"Loss in epoch 1 is 30.0\nLoss in epoch 11 is 1.1627856492996216\nLoss in epoch 21 is 0.0450688973069191\nLoss in epoch 31 is 0.0017468547448515892\nLoss in epoch 41 is 6.770494655938819e-05\nLoss in epoch 51 is 2.6243997126584873e-06\nLoss in epoch 61 is 1.0175587306093803e-07\nLoss in epoch 71 is 3.9741685498029256e-09\nLoss in epoch 81 is 1.4670220593870908e-10\nLoss in epoch 91 is 5.076827847005916e-12\nPredicted value of 7.5 is 14.99999713897705\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}